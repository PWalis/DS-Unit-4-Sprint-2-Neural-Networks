{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "# Backpropagation Practice\n",
    "\n",
    "Implement a 3 input, 4 node hidden-layer, 1 output node Multilayer Perceptron on the following dataset:\n",
    "\n",
    "| x1 | x2 | x3 | y |\n",
    "|----|----|----|---|\n",
    "| 0  | 0  | 1  | 0 |\n",
    "| 0  | 1  | 1  | 1 |\n",
    "| 1  | 0  | 1  | 1 |\n",
    "| 0  | 1  | 0  | 1 |\n",
    "| 1  | 0  | 0  | 1 |\n",
    "| 1  | 1  | 1  | 0 |\n",
    "| 0  | 0  | 0  | 0 |\n",
    "\n",
    "If you look at the data you'll notice that the first two columns behave like an XOR gate while the last column is mostly just noise. Remember that creating an XOR gate was what the perceptron was criticized for not being able to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 1, 1],\n",
       "       [1, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 1, 1],\n",
       "       [0, 0, 0]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array(([0,0,1],\n",
    "              [0,1,1],\n",
    "              [1,0,1],\n",
    "              [0,1,0],\n",
    "              [1,0,0],\n",
    "              [1,1,1],\n",
    "              [0,0,0]))\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0], [1], [1], [1], [1], [0], [0]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [[0],[1],[1],[1],[1],[0],[0]]\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEREYT-3wI1f"
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Set up Architetecture\n",
    "        self.inputs = 3\n",
    "        self.hiddenNodes = 4\n",
    "        self.outputNodes = 1\n",
    "\n",
    "        #Initial weights\n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes) #3x3\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes) #3x1\n",
    "\n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "\n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        \"\"\"\n",
    "\n",
    "        #Weighted sume of inputs and hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "\n",
    "        #Acivations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "\n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "\n",
    "        #Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "\n",
    "        return self.activated_output\n",
    "\n",
    "    def backward(self, X, y, o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        self.o_error = y - o #error in output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o) # apply derivative of sigmoid to error\n",
    "\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T) # z2 error: how much our hidden layer weights were off\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)\n",
    "\n",
    "        self.weights1 += X.T.dot(self.z2_delta) #Adjust first set (input => hidden) weights\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta) #adjust second set (hidden => output) weights\n",
    "\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.train(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights of input to hidden layer\n",
      " [[ 0.31595728 -0.51599277  0.29775516  0.87359247]\n",
      " [ 0.42073051 -0.75039658 -0.91340258  3.06564755]\n",
      " [ 1.76486291 -1.03908122  2.2384044  -0.39257116]]\n",
      "Weights of hidden layer to output\n",
      " [[0.67411202]\n",
      " [0.83005809]\n",
      " [0.4089413 ]\n",
      " [0.77653473]]\n",
      "Error\n",
      " [[-0.86678948]\n",
      " [ 0.09829883]\n",
      " [ 0.11602697]\n",
      " [ 0.1368531 ]\n",
      " [ 0.1406425 ]\n",
      " [-0.90451058]\n",
      " [-0.83584048]]\n",
      "Predicted Output: \n",
      " [[0.81382278]\n",
      " [0.85498639]\n",
      " [0.83241737]\n",
      " [0.82232592]\n",
      " [0.81497412]\n",
      " [0.85749394]\n",
      " [0.79328197]]\n"
     ]
    }
   ],
   "source": [
    "print('Weights of input to hidden layer\\n', nn.weights1)\n",
    "print('Weights of hidden layer to output\\n', nn.weights2)\n",
    "print('Error\\n', nn.o_error)\n",
    "print('Predicted Output: \\n', str(nn.feed_forward(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------EPOCH 1---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0], [1], [1], [1], [1], [0], [0]]\n",
      "Predicted Output: \n",
      " [[0.66199056]\n",
      " [0.67551447]\n",
      " [0.70251064]\n",
      " [0.70932202]\n",
      " [0.73110769]\n",
      " [0.70680889]\n",
      " [0.69644147]]\n",
      "Loss: \n",
      " 0.2533469402604044\n",
      "+---------EPOCH 2---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0], [1], [1], [1], [1], [0], [0]]\n",
      "Predicted Output: \n",
      " [[0.64026436]\n",
      " [0.65227657]\n",
      " [0.67736693]\n",
      " [0.68364426]\n",
      " [0.70467853]\n",
      " [0.68124781]\n",
      " [0.67226559]]\n",
      "Loss: \n",
      " 0.24832535340051803\n",
      "+---------EPOCH 3---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0], [1], [1], [1], [1], [0], [0]]\n",
      "Predicted Output: \n",
      " [[0.62294155]\n",
      " [0.6337053 ]\n",
      " [0.65706565]\n",
      " [0.66244221]\n",
      " [0.68278817]\n",
      " [0.66059431]\n",
      " [0.65231879]]\n",
      "Loss: \n",
      " 0.24518645712733697\n",
      "+---------EPOCH 4---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0], [1], [1], [1], [1], [0], [0]]\n",
      "Predicted Output: \n",
      " [[0.60959567]\n",
      " [0.61941224]\n",
      " [0.64135358]\n",
      " [0.64569718]\n",
      " [0.66549244]\n",
      " [0.64462547]\n",
      " [0.63653519]]\n",
      "Loss: \n",
      " 0.24331800455240757\n",
      "+---------EPOCH 5---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0], [1], [1], [1], [1], [0], [0]]\n",
      "Predicted Output: \n",
      " [[0.59956458]\n",
      " [0.60872679]\n",
      " [0.6295828 ]\n",
      " [0.63290737]\n",
      " [0.65230646]\n",
      " [0.63270234]\n",
      " [0.62441453]]\n",
      "Loss: \n",
      " 0.242233550902298\n",
      "+---------EPOCH 500---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0], [1], [1], [1], [1], [0], [0]]\n",
      "Predicted Output: \n",
      " [[0.05390468]\n",
      " [0.76093985]\n",
      " [0.76045941]\n",
      " [0.78578881]\n",
      " [0.78532958]\n",
      " [0.60787409]\n",
      " [0.05749895]]\n",
      "Loss: \n",
      " 0.08317457587002668\n",
      "+---------EPOCH 1000---------+\n",
      "Input: \n",
      " [[0 0 1]\n",
      " [0 1 1]\n",
      " [1 0 1]\n",
      " [0 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [0 0 0]]\n",
      "Actual Output: \n",
      " [[0], [1], [1], [1], [1], [0], [0]]\n",
      "Predicted Output: \n",
      " [[0.02463482]\n",
      " [0.81023744]\n",
      " [0.81020978]\n",
      " [0.81354173]\n",
      " [0.81352173]\n",
      " [0.43263414]\n",
      " [0.0249133 ]]\n",
      "Loss: \n",
      " 0.04713869094713757\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    if (i+1 in [1,2,3,4,5]) or ((i+1) % 500 ==0):\n",
    "        print('+' + '---' * 3 + f'EPOCH {i+1}' + '---'*3 + '+')\n",
    "        print('Input: \\n', X)\n",
    "        print('Actual Output: \\n', y)\n",
    "        print('Predicted Output: \\n', str(nn.feed_forward(X)))\n",
    "        print(\"Loss: \\n\", str(np.mean(np.square(y - nn.feed_forward(X)))))\n",
    "    nn.train(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8b-r70o8p2Dm"
   },
   "source": [
    "## Try building/training a more complex MLP on a bigger dataset.\n",
    "\n",
    "Use the [MNIST dataset](http://yann.lecun.com/exdb/mnist/) to build the cannonical handwriting digit recognizer and see what kind of accuracy you can achieve. \n",
    "\n",
    "If you need inspiration, the internet is chalk-full of tutorials, but I want you to see how far you can get on your own first. I've linked to the original MNIST dataset above but it will probably be easier to download data through a neural network library. If you reference outside resources make sure you understand every line of code that you're using from other sources, and share with your fellow students helpful resources that you find.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5MOPtYdk1HgA"
   },
   "outputs": [],
   "source": [
    "from mlxtend.data import loadlocal_mnist\n",
    "\n",
    "X, y = loadlocal_mnist(\n",
    "        images_path='/Users/Patrick/Desktop/Repos/DS-Unit-4-Sprint-2-Neural-Networks/module2-backpropagation/data/train-images.idx3-ubyte', \n",
    "        labels_path='/Users/Patrick/Desktop/Repos/DS-Unit-4-Sprint-2-Neural-Networks/module2-backpropagation/data/train-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = []\n",
    "for i in y:\n",
    "    train_labels.append([i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fac = 0.99 / 255\n",
    "\n",
    "train_imgs = np.asfarray(X[:, :]) * fac + 0.01\n",
    "train_labels = np.asfarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.02164706, 0.07988235, 0.07988235,\n",
       "       0.07988235, 0.49917647, 0.538     , 0.68941176, 0.11094118,\n",
       "       0.65447059, 1.        , 0.96894118, 0.50305882, 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.12647059, 0.14976471, 0.37494118, 0.60788235,\n",
       "       0.67      , 0.99223529, 0.99223529, 0.99223529, 0.99223529,\n",
       "       0.99223529, 0.88352941, 0.67776471, 0.99223529, 0.94952941,\n",
       "       0.76705882, 0.25847059, 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.20023529, 0.934     ,\n",
       "       0.99223529, 0.99223529, 0.99223529, 0.99223529, 0.99223529,\n",
       "       0.99223529, 0.99223529, 0.99223529, 0.98447059, 0.37105882,\n",
       "       0.32835294, 0.32835294, 0.22741176, 0.16141176, 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.07988235, 0.86023529, 0.99223529, 0.99223529,\n",
       "       0.99223529, 0.99223529, 0.99223529, 0.77870588, 0.71658824,\n",
       "       0.96894118, 0.94564706, 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.32058824, 0.61564706, 0.42541176, 0.99223529, 0.99223529,\n",
       "       0.80588235, 0.05270588, 0.01      , 0.17694118, 0.60788235,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.06435294,\n",
       "       0.01388235, 0.60788235, 0.99223529, 0.35941176, 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.54964706,\n",
       "       0.99223529, 0.74764706, 0.01776471, 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.05270588, 0.74764706, 0.99223529,\n",
       "       0.28176471, 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.14588235, 0.94564706, 0.88352941, 0.63117647,\n",
       "       0.42929412, 0.01388235, 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.32447059, 0.94176471, 0.99223529, 0.99223529, 0.472     ,\n",
       "       0.10705882, 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.18470588,\n",
       "       0.73211765, 0.99223529, 0.99223529, 0.59235294, 0.11482353,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.07211765, 0.37105882,\n",
       "       0.98835294, 0.99223529, 0.736     , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.97670588, 0.99223529,\n",
       "       0.97670588, 0.25847059, 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.18858824, 0.51470588,\n",
       "       0.72047059, 0.99223529, 0.99223529, 0.81364706, 0.01776471,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.16141176,\n",
       "       0.58458824, 0.89905882, 0.99223529, 0.99223529, 0.99223529,\n",
       "       0.98058824, 0.71658824, 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.10317647, 0.45258824, 0.868     , 0.99223529, 0.99223529,\n",
       "       0.99223529, 0.99223529, 0.79035294, 0.31282353, 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.09929412, 0.26623529, 0.83694118, 0.99223529,\n",
       "       0.99223529, 0.99223529, 0.99223529, 0.77870588, 0.32447059,\n",
       "       0.01776471, 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.07988235, 0.67388235, 0.86023529,\n",
       "       0.99223529, 0.99223529, 0.99223529, 0.99223529, 0.76705882,\n",
       "       0.32058824, 0.04494118, 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.22352941, 0.67776471,\n",
       "       0.88741176, 0.99223529, 0.99223529, 0.99223529, 0.99223529,\n",
       "       0.95729412, 0.52635294, 0.05270588, 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.538     , 0.99223529, 0.99223529, 0.99223529,\n",
       "       0.83305882, 0.53411765, 0.52247059, 0.07211765, 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      , 0.01      ,\n",
       "       0.01      , 0.01      , 0.01      , 0.01      ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_imgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 0., 4., ..., 5., 6., 8.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  5\n",
       "1  0\n",
       "2  4\n",
       "3  1\n",
       "4  9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "train_labels_df = pd.read_csv('./data/train_labels.csv', header=None)\n",
    "train_labels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "dummy_labels = pd.get_dummies(train_labels_df[0])\n",
    "label_matrix = dummy_labels.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4  5  6  7  8  9\n",
       "0  0  0  0  0  0  1  0  0  0  0\n",
       "1  1  0  0  0  0  0  0  0  0  0\n",
       "2  0  0  0  0  1  0  0  0  0  0\n",
       "3  0  1  0  0  0  0  0  0  0  0\n",
       "4  0  0  0  0  0  0  0  0  0  1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dummy_labels.columns:\n",
    "    dummy_labels[i] = dummy_labels[i].map({0:0.01, 1:0.99})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2     3     4     5     6     7     8     9\n",
       "0  0.01  0.01  0.01  0.01  0.01  0.99  0.01  0.01  0.01  0.01\n",
       "1  0.99  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01\n",
       "2  0.01  0.01  0.01  0.01  0.99  0.01  0.01  0.01  0.01  0.01\n",
       "3  0.01  0.99  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01\n",
       "4  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.01  0.99"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Patrick\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_matrix = dummy_labels.as_matrix()\n",
    "label_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01, 0.01, 0.01, 0.01, 0.01, 0.99, 0.01, 0.01, 0.01, 0.01])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Set up Architetecture\n",
    "        self.inputs = 784\n",
    "        self.hiddenNodes = 256\n",
    "        self.outputNodes = 10\n",
    "\n",
    "        #Initial weights\n",
    "        self.weights1 = np.random.randn(self.inputs, self.hiddenNodes) #784x16\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes) #16x10\n",
    "\n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "\n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        \"\"\"\n",
    "\n",
    "        #Weighted sume of inputs and hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "\n",
    "        #Acivations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "\n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "\n",
    "        #Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "\n",
    "        return self.activated_output\n",
    "\n",
    "    def backward(self, X, y, o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        self.o_error = y - o #error in output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o) # apply derivative of sigmoid to error\n",
    "\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T) # z2 error: how much our hidden layer weights were off\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)\n",
    "\n",
    "        self.weights1 += X.T.dot(self.z2_delta) #Adjust first set (input => hidden) weights\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta) #adjust second set (hidden => output) weights\n",
    "\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('weights2\\n', nn.weights2[1])\n",
    "    print('error\\n', str(np.mean(np.square(label_matrix - nn.feed_forward(train_imgs)))))\n",
    "    nn.train(train_imgs, label_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.train(train_imgs, label_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.8821000000000008'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(np.mean(np.square(label_matrix - nn.feed_forward(train_imgs))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = nn.feed_forward(train_imgs[0])\n",
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2414308 , 0.5787545 , 0.32720721, 0.10058537, 0.33339977,\n",
       "       0.6936197 , 0.65226717, 0.99433368, 0.39810497, 0.34569134])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.weights2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        # Set up Architetecture\n",
    "        self.inputs = 784\n",
    "        self.hiddenNodes = 256\n",
    "        self.outputNodes = 10\n",
    "\n",
    "        #Initial weights\n",
    "        self.weights1 = np.random.rand(self.inputs, self.hiddenNodes) #784x16\n",
    "        self.weights2 = np.random.rand(self.hiddenNodes, self.outputNodes) #16x10\n",
    "\n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1+np.exp(-s))\n",
    "\n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def feed_forward(self, X):\n",
    "        \"\"\"\n",
    "        Calculate the NN inference using feed forward.\n",
    "        \"\"\"\n",
    "\n",
    "        #Weighted sume of inputs and hidden layer\n",
    "        self.hidden_sum = np.dot(X, self.weights1)\n",
    "\n",
    "        #Acivations of weighted sum\n",
    "        self.activated_hidden = self.sigmoid(self.hidden_sum)\n",
    "\n",
    "        # Weight sum between hidden and output\n",
    "        self.output_sum = np.dot(self.activated_hidden, self.weights2)\n",
    "\n",
    "        #Final activation of output\n",
    "        self.activated_output = self.sigmoid(self.output_sum)\n",
    "\n",
    "        return self.activated_output\n",
    "\n",
    "    def backward(self, X, y, o):\n",
    "        \"\"\"\n",
    "        Backward propagate through the network\n",
    "        \"\"\"\n",
    "        self.o_error = y - o #error in output\n",
    "        self.o_delta = self.o_error * self.sigmoidPrime(o) # apply derivative of sigmoid to error\n",
    "\n",
    "        self.z2_error = self.o_delta.dot(self.weights2.T) # z2 error: how much our hidden layer weights were off\n",
    "        self.z2_delta = self.z2_error*self.sigmoidPrime(self.activated_hidden)\n",
    "\n",
    "        self.weights1 += X.T.dot(self.z2_delta) #Adjust first set (input => hidden) weights\n",
    "        self.weights2 += self.activated_hidden.T.dot(self.o_delta) #adjust second set (hidden => output) weights\n",
    "\n",
    "    def train(self, X, y):\n",
    "        o = self.feed_forward(X)\n",
    "        self.backward(X, y, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o = nn.feed_forward(train_imgs)\n",
    "o[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o_error, this is target minus prpedicted output\n",
      " [-0.99 -0.99 -0.99 -0.99 -0.99 -0.01 -0.99 -0.99 -0.99 -0.99]\n",
      "o_delta, this is o_error times the sigmoid function of output\n",
      " [-0. -0. -0. -0. -0. -0. -0. -0. -0. -0.]\n",
      "z2_error, the dot product of the transpose of weigfhts2(hidden layer to output layer)\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "z2_delta, product of z2_error and the sigmoidPrime of activated_hidden layer\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "weights1 first 20\n",
      " [0.58172878 0.51862596 0.76963597 0.85157964 0.37114321 0.34818226\n",
      " 0.04076841 0.56205694 0.04061429 0.88965714 0.59237385 0.57571345\n",
      " 0.92265943 0.6732554  0.84882632 0.20645716 0.94325257 0.69127232\n",
      " 0.62209258 0.29416218]\n",
      "weights2 first 20\n",
      " [0.83936729 0.6073244  0.89570978 0.85881807 0.68020827 0.14756772\n",
      " 0.60495943 0.91290711 0.76137411 0.03160807]\n",
      "weights1 updated\n",
      " [0.58172878 0.51862596 0.76963597 0.85157964 0.37114321 0.34818226\n",
      " 0.04076841 0.56205694 0.04061429 0.88965714 0.59237385 0.57571345\n",
      " 0.92265943 0.6732554  0.84882632 0.20645716 0.94325257 0.69127232\n",
      " 0.62209258 0.29416218]\n",
      "weights2 updated\n",
      " [0.83936729 0.6073244  0.89570978 0.85881807 0.68020827 0.14756772\n",
      " 0.60495943 0.91290711 0.76137411 0.03160807]\n"
     ]
    }
   ],
   "source": [
    "o_error = label_matrix - o #error in output\n",
    "o_delta = o_error * nn.sigmoidPrime(o) # apply derivative of sigmoid to error\n",
    "print('o_error, this is target minus prpedicted output\\n', o_error[0])\n",
    "print('o_delta, this is o_error times the sigmoid function of output\\n', o_delta[0])\n",
    "\n",
    "z2_error = o_delta.dot(nn.weights2.T) # z2 error: how much our hidden layer weights were off\n",
    "z2_delta = z2_error*nn.sigmoidPrime(nn.activated_hidden)\n",
    "print('z2_error, the dot product of the transpose of weigfhts2(hidden layer to output layer)\\n', z2_error[0])\n",
    "print('z2_delta, product of z2_error and the sigmoidPrime of activated_hidden layer\\n', z2_delta[0])\n",
    "\n",
    "print('weights1 first 20\\n', nn.weights1[0][:20])\n",
    "print('weights2 first 20\\n', nn.weights2[0][:20])\n",
    "nn.weights1 += X.T.dot(z2_delta) #Adjust first set (input => hidden) weights\n",
    "nn.weights2 += nn.activated_hidden.T.dot(o_delta) #adjust second set (hidden => output) weights\n",
    "print('weights1 updated\\n', nn.weights1[0][:20])\n",
    "print('weights2 updated\\n', nn.weights2[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwlRJSfBlCvy"
   },
   "source": [
    "## Stretch Goals: \n",
    "\n",
    "- Implement Cross Validation model evaluation on your MNIST implementation \n",
    "- Research different [Gradient Descent Based Optimizers](https://keras.io/optimizers/)\n",
    " - [Siraj Raval the evolution of gradient descent](https://www.youtube.com/watch?v=nhqo0u1a6fw)\n",
    "- Build a housing price estimation model using a neural network. How does its accuracy compare with the regression models that we fit earlier on in class?"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "LS_DS_432_Backprop_Assignment.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
